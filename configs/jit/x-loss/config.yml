model:
  checkpoint_path:

  dtype: bfloat16

  denoiser:
    context_dim: 768
    hidden_size: 768
    num_heads: 12
    rope_axes_dims: [16, 24, 24]
    rope_axes_lens: [256, 128, 128]
    rope_zero_centered: [False, True, True]

  context_encoder:
    type: "class"
    label2id_map_path: "models/jit-animeface-labels.json"

  max_token_length: 64
  noise_scale: 1.0

  loss_target: image
  timestep_sampling: scale_shift_sigmoid # oirignal impl is scale_shift_sigmoid

# peft:
#   - config: # Linear
#       type: lora
#       rank: 8
#       alpha: 1.0
#       dropout: 0.0

#       dtype: bfloat16

#     include_keys:
#       # - "time_embed" # Input
#       # - "label_emb" # Input
#       - "attn1" # Transformer
#       - "attn2" # Transformer
#       - ".ff." # Transformer
#       # - "proj_in" # Transformer
#       # - "proj_out" # Transformer
#       # - "emb_layers" # ResBlock
#     exclude_keys: ["text_encoder", "vae"]

#   # - config: # Conv2d
#   #     type: lora
#   #     rank: 16
#   #     alpha: 1.0
#   #     dropout: 0.0

#   #     dtype: bfloat16

#   #   include_keys:
#   #     - "in_layers" # ResBlock
#   #     - "out_layers" # ResBlock
#   #   exclude_keys: ["text_encoder", "vae"]

dataset:
  supported_extensions: [".webp"]
  folder: "../anime-face-dev/data/animeface-20s-1.8M"
  tags_folder: "../anime-face-dev/data/tags"
  metadata_extension: ".tags.json"
  num_repeats: 1
  batch_size: 4

  bucket_base_size: 256
  step: 128
  min_size: 256
  do_upscale: false

  caption_processors:
    - type: shuffle
      separator: " "

optimizer:
  name: "schedulefree.RAdamScheduleFree"
  # name: "bitsandbytes.optim.AdamW8bit"
  args:
    lr: 0.0001

scheduler:
  # name: "torch.optim.lr_scheduler.ConstantLR"
  # args: {}

tracker:
  project_name: "JiT/AnimeFace/01"
  loggers:
    - wandb

saving:
  strategy:
    per_epochs: 1
    per_steps: null
    save_last: true

  callbacks:
    - type: "safentensors" # or "hf_hub" to push to hub
      name: "jit-animeface"
      save_dir: "./output/jit-01"

preview:
  strategy:
    per_epochs: 1
    per_steps: 100

  callbacks:
    - type: "local"
      save_dir: "./output/jit-01/preview"

  data:
    # you should change the project dir
    path: "./configs/jit/x-loss/preview.yml"

seed: 42
num_train_epochs: 100

trainer:
  debug_mode: "1step"

  gradient_checkpointing: true
  gradient_accumulation_steps: 1

  torch_compile: false
  torch_compile_args:
    backend: "eager"
    mode: default
    fullgraph: false

  fp32_matmul_precision: "high"
